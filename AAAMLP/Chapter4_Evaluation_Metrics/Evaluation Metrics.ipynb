{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d131ae0",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/frank77/Python-Data-Analysis/AAAMLP/Chapter4_Evaluation_Metrics/Evaluation Metrics.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b7ea81",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Evaluation Metrics\n",
    "## Classifcation\n",
    "- Accuracy\n",
    "- Precision(P)\n",
    "- Recall (R)\n",
    "- F1 Score(F1)\n",
    "- AUC\n",
    "- Log Loss\n",
    "- Precision at k (P@K)\n",
    "- Average Precision at k (AP@K)\n",
    "- Mean Average Precision at k (MAP@K)\n",
    "## Regression\n",
    "- Mean Absolute Error (MAE)\n",
    "- Mean Squared Error (MSE)\n",
    "- Root Mean Squared Error (RMSE)\n",
    "- Root Mean Squared Logarithmic Error (RMSLE)\n",
    "- Mean Percentage Error (MPE)\n",
    "- Mean Absolute Percentage Error (MAPE)\n",
    "- R2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb597155",
   "metadata": {},
   "source": [
    "## Classification Metrics\n",
    "\n",
    "**When we have equal number of positive and negative samples in a binary classification, we generally use *accuracy， precision, recall and f1***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404bcabd",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8107a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    count_val = 0\n",
    "    for i, j in zip(y_true, y_pred):\n",
    "        if i == j:\n",
    "            count_val += 1\n",
    "    \n",
    "    return count_val / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9320db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "l1 = [0,1,1,1,0,0,0,1]\n",
    "l2 = [0,1,0,1,0,1,0,0]\n",
    "\n",
    "\n",
    "print(accuracy(l1,l2))\n",
    "\n",
    "#sklearn function to calculate  accuracy \n",
    "metrics.accuracy_score(l1,l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686e188b",
   "metadata": {},
   "source": [
    "**If the dataset is skewed, accuracy is not an good choice. we need to use precision.**\n",
    "\n",
    "### Precision:\n",
    "- **True Positive(TP)**\n",
    "- **True Negtive(TN)**\n",
    "- **False Positive(FP)**\n",
    "- **False Negtive(FN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2089b085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TP(y_true, y_pred):\n",
    "    tp = 0\n",
    "    for y, p in zip(y_true, y_pred):\n",
    "        if y == 1 and p == 1:\n",
    "            tp += 1\n",
    "    \n",
    "    return tp \n",
    "\n",
    "def TN(y_true, y_pred):\n",
    "    tn = 0\n",
    "    for y, p in zip(y_true, y_pred):\n",
    "        if y == 0 and p == 0:\n",
    "            tn += 1\n",
    "    \n",
    "    return tn \n",
    "\n",
    "def FP(y_true, y_pred):\n",
    "    fp = 0\n",
    "    for y, p in zip(y_true, y_pred):\n",
    "        if y == 0 and p == 1:\n",
    "            fp += 1\n",
    "    \n",
    "    return fp\n",
    "\n",
    "def FN(y_true, y_pred):\n",
    "    fn = 0\n",
    "    for y, p in zip(y_true, y_pred):\n",
    "        if y == 1 and p == 0:\n",
    "            fn += 1\n",
    "    \n",
    "    return fn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93eff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [0,1,1,1,0,0,0,1]\n",
    "l2 = [0,1,0,1,0,1,0,0]\n",
    "\n",
    "print(TP(l1, l2), FP(l1, l2), FN(l1, l2), TN(l1, l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8567607",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{gather}\n",
    "\\text{Accuracy} = \\frac{(TP + TN)}{TP+TN+FP+FN}, \\quad\n",
    "\\text{Precision} = \\frac{TP}{TP+FP}, \\quad\n",
    "\\text{Recall} = \\frac{TP}{TP+FN}\n",
    "\\end{gather}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178b72ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    tp = TP(y_true, y_pred)\n",
    "    tn = TN(y_true, y_pred)\n",
    "    fp = FP(y_true, y_pred)\n",
    "    fn = FN(y_true, y_pred)\n",
    "    \n",
    "    return (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    tp = TP(y_true, y_pred)\n",
    "    fp = FP(y_true, y_pred)\n",
    "    \n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    tp = TP(y_true, y_pred)\n",
    "    fn = FN(y_true, y_pred)\n",
    "    \n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff7e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [0,1,1,1,0,0,0,1]\n",
    "l2 = [0,1,0,1,0,1,0,0]\n",
    "\n",
    "print(accuracy(l1, l2), precision(l1, l2), recall(l1, l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4742b35c",
   "metadata": {},
   "source": [
    "### Precision Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296bbdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0]\n",
    "\n",
    "y_pred = [.02638412, 0.11114267, 0.31620708, \n",
    "0.0490937, 0.0191491, 0.17554844, \n",
    "0.15952202, 0.03819563, 0.11639273, \n",
    "0.079377, 0.08584789, 0.39095342, \n",
    "0.27259048, 0.03447096, 0.04644807, \n",
    "0.03543574, 0.18521942, 0.05934905, \n",
    "0.61977213, 0.33056815]\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "#how we assumed these threshold is a long story\n",
    "\n",
    "thresholds = [0.0490937, 0.05934905, 0.079377,\n",
    "              0.08584789, 0.11114267, 0.11639273, \n",
    "              0.15952202, 0.17554844, 0.18521942, \n",
    "              0.27259048, 0.31620708, 0.33056815, \n",
    "              0.39095342, 0.61977213] \n",
    "\n",
    "\n",
    "#for every threshold, calculate predictions in binary\n",
    "#and append their calculated precision and recalls\n",
    "#to their respective lists\n",
    "for i in thresholds:\n",
    "    temp_prediction = [1 if x >= i else 0 for x in y_pred]\n",
    "    p = precision(y_true,temp_prediction)\n",
    "    r = recall(y_true , temp_prediction)\n",
    "    precisions.append(p)\n",
    "    recalls.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be146fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (7,7))\n",
    "plt.plot(recalls,precisions)\n",
    "plt.xlabel(\"Recall\" ,fontsize = 15)\n",
    "plt.ylabel(\"Precision\",  fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318ca0d3",
   "metadata": {},
   "source": [
    "**Choosing a threshhod can be quite challenging**\n",
    "### F1 Score\n",
    "$$\n",
    "\\begin{gather}\n",
    "\\text{F1} = \\frac{2PR}{P + R}, \\quad  \n",
    "\\text{F1} = \\frac{2TP}{2TP + FP +FN} \n",
    "\\end{gather}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07f4fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "\n",
    "    score = 2 * p * r / (p + r)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74c9ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0,0,0,1,0,0,0,0,0,0,\n",
    "         1,0,0,0,0,0,0,0,1,0]\n",
    "\n",
    "\n",
    "y_pred = [0,0,1,0,0,0,1,0,0,0,\n",
    "         1,0,0,0,0,0,0,0,1,0]\n",
    "\n",
    "f1(y_true,y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ced48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using scikit learn \n",
    "metrics.f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783f0875",
   "metadata": {},
   "source": [
    "### TPR - True Positive Rate = Recall\n",
    "$$\n",
    "TPR = \\frac{TP}{TP + FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f282e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpr(y_true, y_pred):\n",
    "    tp = TP(y_true, y_pred)\n",
    "    fn = FN(y_true, y_pred)\n",
    "    \n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1d13fc",
   "metadata": {},
   "source": [
    "### FPR - False Positive Rate\n",
    "$$\n",
    "FPR = \\frac{FP}{FP + TN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b227e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fpr(y_true, y_pred):\n",
    "    fp = FP(y_true, y_pred)\n",
    "    tn = TN(y_true, y_pred)\n",
    "\n",
    "    return fp / (fp + tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576cd8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty lists to store tpr\n",
    "#and fpr values\n",
    "\n",
    "tpr_list = []\n",
    "fpr_list = []\n",
    "\n",
    "#actual targets\n",
    "y_true = [0,0,0,0,1,0,1,0,0,1,0,1,0,0,1]\n",
    "\n",
    "\n",
    "#predicted probabilities of the sample being 1\n",
    "y_pred = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05, 0.9, 0.5, 0.3, 0.66, 0.3, 0.2, 0.85, 0.15, 0.99] \n",
    "\n",
    "#handmade thresholds \n",
    "thresholds = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.99, 1.0] \n",
    "\n",
    "\n",
    "#loop over all thresholds \n",
    "for thresh in thresholds:\n",
    "    #calculate predictions for a given threshold\n",
    "    temp_pred = [1 if x >= thresh else 0 for x in y_pred]\n",
    "    #calculate tpr\n",
    "    temp_tpr = tpr(y_true, temp_pred)\n",
    "    #calculate fpr \n",
    "    temp_fpr = fpr(y_true, temp_pred) \n",
    "    #append tpr and fpr to lists \n",
    "    tpr_list.append(temp_tpr) \n",
    "    fpr_list.append(temp_fpr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5805478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "values = {\"threshold\":thresholds,\"tpr\":tpr_list,\"fpr\":fpr_list}\n",
    "values  = pd.DataFrame(values)\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2185fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7,7))\n",
    "plt.plot(fpr_list, tpr_list, lw = 3)\n",
    "plt.fill_between(fpr_list, tpr_list, alpha = 0.4)\n",
    "plt.xlabel(\"FPR\", fontsize = 15)\n",
    "plt.ylabel(\"TPR\", fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381d67bc",
   "metadata": {},
   "source": [
    "- this curve is also known as Reciver Operating Characteristic (**ROC**)\n",
    "- Area Under ROC curve is known as **AUC**\n",
    "- they are often used when we have **skewed binary target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dabb638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_true = [0, 0, 0, 0, 1, 0, 1,0, 0, 1, 0, 1, 0, 0, 1] \n",
    "\n",
    "y_pred = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05, 0.9, 0.5, 0.3, 0.66, 0.3, 0.2, 0.85, 0.15, 0.99] \n",
    "\n",
    "metrics.roc_auc_score(y_true, y_pred)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a363f42",
   "metadata": {},
   "source": [
    "### Log Loss\n",
    "$$\n",
    "\\text{Log Loss} = -1.0 \\times (target - \\log(pred) + (1 - target) * \\log(1 - pred))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345762a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def log_loss(y_true, y_pred):\n",
    "    epsilon = 1e-15 \n",
    "    loss = 0\n",
    "    n = len(y_true)\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        yp = np.clip(yp, epsilon, 1 - epsilon) \n",
    "        loss += yt * np.log(yp) + (1 - yt) * np.log(1 - yp)\n",
    "    \n",
    "    loss = - loss / n\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e6a499",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0, 0, 0, 0, 1, 0, 1,0, 0, 1, 0, 1, 0, 0, 1]\n",
    "y_proba = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05,0.9, 0.5, 0.3, 0.66, 0.3, 0.2, 0.85, 0.15, 0.99]\n",
    "\n",
    "log_loss(y_true,y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e82ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "metrics.log_loss(y_true,y_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6f0c2c",
   "metadata": {},
   "source": [
    "### Precision For Multi-Class Classification\n",
    "- **Macro Average Precision** $$ \\text{Macro Precision}=\\frac{1}{N}\\sum_{i=1}^N Precision_i $$\n",
    "- **Micro Average Precision** $$ \\text{Micro Precision}=\\frac{\\sum_{i=1}^N TP_i}{\\sum_{i=1}^N (TP_i + FP_i)} $$\n",
    "- **Weighted Precision** $$ \\text{Weighted Precision}=\\frac{\\sum_{i=1}^N (n_i * Precision_i)}{\\sum_{i=1}^N N_i} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be717987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_precision(y_true, y_pred):\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    precisions = []\n",
    "    for cls in range(num_classes):\n",
    "        tp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == cls and yp == cls)\n",
    "        fp = sum(1 for yt, yp in zip(y_true, y_pred) if yt != cls and yp == cls)\n",
    "        precision_cls = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        precisions.append(precision_cls)\n",
    "    \n",
    "    return sum(precisions) / num_classes\n",
    "\n",
    "def micro_precision(y_true, y_pred):\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    for cls in range(num_classes):\n",
    "        tp += sum(1 for yt, yp in zip(y_true, y_pred) if yt == cls and yp == cls)\n",
    "        fp += sum(1 for yt, yp in zip(y_true, y_pred) if yt != cls and yp == cls)\n",
    "    \n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def weighted_precision(y_true, y_pred):\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    precisions = []\n",
    "    weights = []\n",
    "    for cls in range(num_classes):\n",
    "        tp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == cls and yp == cls)\n",
    "        fp = sum(1 for yt, yp in zip(y_true, y_pred) if yt != cls and yp == cls)\n",
    "        precision_cls = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        weight_cls = sum(1 for yt in y_true if yt == cls)\n",
    "        precisions.append(precision_cls * weight_cls)\n",
    "        weights.append(weight_cls)\n",
    "    \n",
    "    return sum(precisions) / sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b2d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics \n",
    "y_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]\n",
    "y_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2] \n",
    "\n",
    "print(macro_precision(y_true, y_pred)) \n",
    "print(metrics.precision_score(y_true, y_pred, average=\"macro\"))\n",
    "\n",
    "print(micro_precision(y_true, y_pred))\n",
    "print(metrics.precision_score(y_true, y_pred, average=\"micro\")) \n",
    "\n",
    "print(weighted_precision(y_true, y_pred))\n",
    "print(metrics.precision_score(y_true, y_pred, average=\"weighted\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6aea5e",
   "metadata": {},
   "source": [
    "## MultiLabel Classification problem\n",
    "In multilabel classificaion each sample can have one or more classes associated with it.\n",
    "Metric for this problem\n",
    "- Precision at K **P@k)**\n",
    "- Average Precision at k **(AP@k)**\n",
    "- Mean Average Precision at k **(MAP@k)**\n",
    "- Log Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d82610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pk(y_true, y_pred, k):\n",
    "    if k == 0:\n",
    "        return 0\n",
    "    y_pred = y_pred[:k]\n",
    "    pred_set = set(y_pred)\n",
    "    true_set = set(y_true)\n",
    "    intersection = pred_set.intersection(true_set)\n",
    "    return len(intersection) / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a526e71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(y_true, y_pred, k):\n",
    "    total_pk = 0\n",
    "    for i in range(1, k + 1):\n",
    "        total_pk += pk(y_true, y_pred, i)\n",
    "    return total_pk / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fb2b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapk(y_true_list, y_pred_list, k):\n",
    "    total_apk = 0\n",
    "    n = len(y_true_list)\n",
    "    for y_true, y_pred in zip(y_true_list, y_pred_list):\n",
    "        total_apk += apk(y_true, y_pred, k)\n",
    "    return total_apk / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d68da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [\n",
    "\t\t[1, 2, 3],\n",
    "\t\t[0, 2],\n",
    "\t\t[1],\n",
    "\t\t[2, 3],\n",
    "\t\t[1, 0],\n",
    "\t\t[]\n",
    "\t]\n",
    "y_pred = [\n",
    "\t\t[0, 1, 2],\n",
    "\t\t[1],\n",
    "\t\t[0, 2, 3],\n",
    "\t\t[2, 3, 4, 0],\n",
    "\t\t[0, 1, 2],\n",
    "\t\t[0]\n",
    "\t]\n",
    "for i in range(len(y_true)):\n",
    "\tfor j in range(1, 4):\n",
    "\t\tprint(\n",
    "\t\t\t\tf\"\"\"\n",
    "\t\t\t\ty_true={y_true[i]},\n",
    "\t\t\t\ty_pred={y_pred[i]},\n",
    "\t\t\t\tAP@{j}={apk(y_true[i], y_pred[i], k=j)}\n",
    "\t\t\t\t\"\"\"\n",
    "\t\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77462f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [\n",
    "\t\t\t[1, 2, 3],\n",
    "\t\t\t[0, 2],\n",
    "\t\t\t[1],\n",
    "\t\t\t[2, 3],\n",
    "\t\t\t[1, 0],\n",
    "\t\t\t[]\n",
    "\t\t]\n",
    "\n",
    "y_pred = [\n",
    "\t\t\t[0, 1, 2],\n",
    "\t\t\t[1],\n",
    "\t\t\t[0, 2, 3],\n",
    "\t\t\t[2, 3, 4, 0],\n",
    "\t\t\t[0, 1, 2],\n",
    "\t\t\t[0]\n",
    "\t\t]\n",
    "\n",
    "\n",
    "print(\"Mean Average Precision @ k:\", mapk(y_true, y_pred, k=1))\n",
    "print(\"Mean Average Precision @ k:\", mapk(y_true, y_pred, k=2))\n",
    "print(\"Mean Average Precision @ k:\", mapk(y_true, y_pred, k=3))\n",
    "print(\"Mean Average Precision @ k:\", mapk(y_true, y_pred, k=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d6eae",
   "metadata": {},
   "source": [
    "**Log Loss** for multilabel classfication is quite easy. You can covert the targets to binary format and then use a log loss for each column. You can take the average of log loss of each column. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53af2fb5",
   "metadata": {},
   "source": [
    "## Metrics for Regression\n",
    "- ### Error\n",
    "- ### Absolute Error\n",
    "- ### Mean Absolute Error(MAE)\n",
    "- ### Mean Squared Error(MSE)\n",
    "- ### Root Mean Squared Error(RMSE)\n",
    "- ### Squared Logarithmic Error(SLE)\n",
    "- ### Mean Squared Logarithmic Error(MSLE)\n",
    "- ### Root Mean Squared Logarithmic Error(RMSLE)\n",
    "- ### Percent Error\n",
    "- ### Mean Absolute Percentage Eoor(MAPE)\n",
    "- ### R2 or R-quared or coefficient of determination\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum^N_{i=1}(y_{t_i} - y_{p_i})^2}{\\sum^N_{i=1}(y_{t_i} - y_{t_{mean}})^2}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e161a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    n = len(y_true)\n",
    "    total_error = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        total_error += (yt - yp) ** 2\n",
    "    return total_error / n\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    n = len(y_true)\n",
    "    total_error = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        total_error += abs(yt - yp)\n",
    "    return total_error / n\n",
    "\n",
    "def msle(y_true, y_pred):\n",
    "    n = len(y_true)\n",
    "    total_error = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        total_error += np.log(1 + yt) - np.log(1 + yp) ** 2\n",
    "    return total_error / n\n",
    "\n",
    "def r2(y_true, y_pred):\n",
    "    mean = mean_true_value = np.mean(y_true)\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        numerator += (yt - yp) ** 2\n",
    "        denominator += (yt - mean) ** 2\n",
    "    \n",
    "    return 1 - (numerator / denominator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74e29bf",
   "metadata": {},
   "source": [
    "## Other Advanced Metric\n",
    "- ### Quadratic Weighted Kappa, also known as QWK. It is also known as Cohen’s kappa\n",
    "**QWK measures the “agreement” between two “ratings”.**\n",
    "- The ratings can be any real numbers in 0 to N. And predictions are also in the same range.\n",
    "- An agreement can be defined as how close these ratings are to each other. So, it’s suitable for a classification problem with N different categories/classes.\n",
    "- If the agreement is high, the score is closer towards 1.0. In the case of low agreement, the score is close to 0.\n",
    "- ### Mattew's Correalation Coefficient(MCC)\n",
    "$$\n",
    "MCC = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP + FP) \\times (TN + FN) \\times (FP + TN) \\times (TP + FN)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff20209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics \n",
    "\n",
    "y_true = [1, 2, 3, 1, 2, 3, 1, 2, 3] \n",
    "y_pred = [2, 1, 3, 1, 2, 3, 3, 1, 2] \n",
    "\n",
    "metrics.cohen_kappa_score(y_true, y_pred, weights=\"quadratic\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acacab4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
